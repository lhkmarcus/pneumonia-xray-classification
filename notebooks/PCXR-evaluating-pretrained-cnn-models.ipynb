{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DenseNet-169 for multi-class classification of CXR images**\n",
    "\n",
    "### Objectives\n",
    "\n",
    "The aim of this notebook is to train a selected pretrained network [TBD] on the Chest X-ray (Pneumonia) data set sourced from Kaggle. The images are composed of CXR scans representing normal lungs, bacterial pneumonia, and viral pneumonia.\n",
    "\n",
    "A **classification head** will be built on top of the chosen network, in which various architectures are developed and evaluated. Several preprocessing techniques are applied, each of which will be evaluated based on the model performance when trained on them. Once trained and evaluated, each version of the network will be further evaluated with the **GRAD-CAM** framework to visualise the regions of importance in the images.\n",
    "\n",
    "The experiment tracking will be conducted with the *Weights and Biases* platform. \n",
    "\n",
    "### Machine Configurations\n",
    "\n",
    "`GPU` : NVIDIA GeForce RTX 4090;\n",
    "`CPU` : AMD Ryzen 7 3700X 8-Core Processor;\n",
    "`VRAM` : 24.0 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version :  2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Suppress TensorFlow messages:\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "print(\"TensorFlow version : \", tf.__version__)  # check TensorFlow version\n",
    "gpu = tf.config.list_physical_devices(\"GPU\")    # check if TensorFlow is using the GPU\n",
    "print(gpu)\n",
    "\n",
    "# Enable memory growth for GPU:\n",
    "try:\n",
    "   tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "   print(\"Memory growth set.\")\n",
    "except:\n",
    "    print(\"GPU runtime already initialised.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure notebook name for WandB:\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"PCXR-evaluating-pretrained-cnn-model\"\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose method for hyperparameter selection:\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish objective to optimise:\n",
    "metric = {\n",
    "    \"name\": \"val_loss\",\n",
    "    \"goal\": \"minimize\"   \n",
    "    }\n",
    "\n",
    "sweep_config[\"metric\"] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose hyperparameters to sweep:\n",
    "param_dict = {\n",
    "    \"batch_size\": {\n",
    "        \"values\": [16, 32, 64]\n",
    "        },\n",
    "    \"optimiser\": {\n",
    "        \"values\": [\"adam\", \"sgd\"]\n",
    "        },\n",
    "    \"fc_layer_size\": {\n",
    "        \"values\": [32, 64, 128, 256]\n",
    "        },\n",
    "    \"dropout_1\": {\n",
    "        \"values\": [0.2, 0.6, 0.8]\n",
    "        },\n",
    "    \"dropout_2\": {\n",
    "        \"values\": [0.2, 0.6, 0.8]\n",
    "        },\n",
    "    \"dropout_3\": {\n",
    "        \"values\": [0.2, 0.6, 0.8]\n",
    "        },\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate constant value for epoch:\n",
    "param_dict.update({\n",
    "    \"epochs\": {\n",
    "        'value': 10}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use uniform distribution for learning rate:\n",
    "param_dict.update({\n",
    "    'learning_rate': {\n",
    "        \"distribution\": \"uniform\",\n",
    "        \"min\": 0.0001,\n",
    "        \"max\": 0.005\n",
    "      }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'bayes',\n",
      " 'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
      " 'parameters': {'batch_size': {'values': [16, 32, 64]},\n",
      "                'dropout_1': {'values': [0.2, 0.6, 0.8]},\n",
      "                'dropout_2': {'values': [0.2, 0.6, 0.8]},\n",
      "                'dropout_3': {'values': [0.2, 0.6, 0.8]},\n",
      "                'epochs': {'value': 10},\n",
      "                'fc_layer_size': {'values': [32, 64, 128, 256]},\n",
      "                'learning_rate': {'distribution': 'uniform',\n",
      "                                  'max': 0.005,\n",
      "                                  'min': 0.0001},\n",
      "                'optimiser': {'values': ['adam', 'sgd']}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datasets(train_dir=\"..\\\\artifacts\\\\train\",\n",
    "                   test_dir=\"..\\\\artifacts\\\\test\",\n",
    "                   class_names=[\"normal\", \"bacteria\", \"virus\"],\n",
    "                   image_size=(300, 300),\n",
    "                   val_split=0.2):\n",
    "    \n",
    "    train_ds, valid_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=train_dir,\n",
    "        label_mode=\"int\",\n",
    "        class_names=class_names,\n",
    "        batch_size=None,\n",
    "        image_size=image_size,\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "        validation_split=val_split,\n",
    "        subset=\"both\")\n",
    "\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=test_dir,\n",
    "        label_mode=\"int\",\n",
    "        class_names=class_names,\n",
    "        batch_size=None,\n",
    "        image_size=image_size,\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "        subset=None)\n",
    "    \n",
    "    return (train_ds, valid_ds, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(fc_layer_size=128, dropout_1=0.2, dropout_2=0.2, dropout_3=0.2):\n",
    "\n",
    "    # Configure image shape for model input shape\n",
    "    IMG_SHAPE = (300, 300)\n",
    "\n",
    "    # Instantiate base model\n",
    "    base_model = tf.keras.applications.densenet.DenseNet169(\n",
    "        input_shape=IMG_SHAPE,\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\")\n",
    "\n",
    "    # Freeze convolutional base\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "\n",
    "    # Add an augmentation layer for additional training data\n",
    "    augment_layer = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomContrast(factor=0.2, seed=42),\n",
    "        tf.keras.layers.RandomBrightness(factor=0.2, seed=42),\n",
    "        tf.keras.layers.RandomRotation(factor=0.01, seed=42)\n",
    "    ])\n",
    "\n",
    "    # Establish model architecture:\n",
    "    x = augment_layer(inputs)\n",
    "    x = tf.keras.applications.densenet.preprocess_input(x)\n",
    "    x = base_model(x, training=False)    # Prevent training of BN layers\n",
    "\n",
    "    # Add custom classification head:\n",
    "    x = tf.keras.layers.MaxPooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_1)(x)  # Prevent overfitting\n",
    "    x = tf.keras.layers.Dense(fc_layer_size, 3, activation=\"relu\", input_shape=(1664,))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_2)(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_3)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(3, activation=\"softmax\")(x)\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimiser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimiser(learning_rate=0.0001, optimiser=\"adam\"):\n",
    "    # Define optimisers:\n",
    "    if optimiser.lower() == \"adam\":\n",
    "        return tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    if optimiser.lower() == \"sgd\":\n",
    "        return tf.keras.optimizers.SGD(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create callback to log custom metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MCC, F1, and AUC scores and log in WandB:\n",
    "class CustomLogCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model, x_val, y_val):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "        # Instantiate standalone metrics:\n",
    "        self._mcc = tf.keras.metrics.Mean(name=\"mcc\")\n",
    "        self._f1score = tf.keras.metrics.Mean(name=\"f1_score\")\n",
    "        self._aucroc = tf.keras.metrics.AUC(name=\"auc_roc\")\n",
    "\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch += 1\n",
    "\n",
    "        self._mcc.reset_state()\n",
    "        self._f1score.reset_state()\n",
    "        self._aucroc.reset_state()\n",
    "\n",
    "        print(\"Generating predictions and computing metrics for Epoch {} \".format(self.epoch))\n",
    "        predictions = self.model.predict(self.x_val)\n",
    "\n",
    "        f1score = f1_score(self.y_val, np.argmax(predictions, axis=-1),\n",
    "                            average=None)\n",
    "        mcc = matthews_corrcoef(self.y_val, np.argmax(predictions, axis=-1))\n",
    "\n",
    "        self._mcc.update_state(mcc)\n",
    "        self._f1score.update_state(f1score)\n",
    "        self._aucroc.update_state(self.y_val, np.argmax(predictions, axis=-1))\n",
    "\n",
    "        print(\"training loss : {} , training acc : {} , mcc score : {}\".format(\n",
    "            logs[\"loss\"], logs[\"accuracy\"], self._mcc.result().numpy()\n",
    "        ))\n",
    "        print(\"aucroc score  : {} , f1 score     : {} \".format(\n",
    "            self._aucroc.result().numpy(), self._f1score.result().numpy()\n",
    "        ))\n",
    "\n",
    "        # Log metrics to WandB:\n",
    "        wandb.log({\"mcc\": self._mcc.result().numpy(),\n",
    "                   \"fmeasure\": self._f1score.result().numpy(),\n",
    "                   \"auc_roc\": self._aucroc.result().numpy()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data sets outside of train function\n",
    "train_ds, valid_ds, _ = build_datasets()\n",
    "\n",
    "# Convert validation data to arrays for custom metric computation:\n",
    "x_val = np.concatenate([x for x, y in valid_ds], axis=0)\n",
    "y_val = np.concatenate([y for x, y in valid_ds], axis=0)\n",
    "\n",
    "def train(model, batch_size=32, epochs=10, learning_rate=0.0001, optimiser_name=\"adam\"):\n",
    "\n",
    "    optimiser = build_optimiser(learning_rate=learning_rate, optimiser=optimiser_name)\n",
    "\n",
    "    # Compile model:\n",
    "    model.compile(optimizer=optimiser,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Fit model:\n",
    "    # Configure callbacks:\n",
    "    callbacks = [CustomLogCallback(model, x_val, y_val),\n",
    "                 wandb.keras.WandbMetricsLogger()]\n",
    "    # Train model:\n",
    "    model.fit(train_ds, epochs=epochs, batch_size=batch_size,\n",
    "              validation_data=valid_ds, callbacks=callbacks, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop training function in sweep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_train(config_defaults=None):\n",
    "    \n",
    "    # Initialise wandb and start run:\n",
    "    with wandb.init(config=config_defaults):\n",
    "        # Specify other experimentation configurations:\n",
    "        wandb.config.architecture = \"DenseNet\"\n",
    "        wandb.config.dataset = \"Chest X-Ray Images (Pneumonia)\"\n",
    "        wandb.config.num_classes = 3\n",
    "        wandb.notes = \"Sweeping through DenseNet model variations\"\n",
    "\n",
    "        model = build_network(wandb.config.fc_layer.size,\n",
    "                              wandb.config.dropout_1,\n",
    "                              wandb.config.dropout_2,\n",
    "                              wandb.config.dropout_3)\n",
    "\n",
    "        train(model=model, batch_size=wandb.config.batch_size,\n",
    "              epochs=wandb.config.epochs, learning_rate=wandb.config.learning_rate,\n",
    "              optimiser_name=wandb.config.optimiser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise sweep and run agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"PCXR-evaluating-pretrained-model-variations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=sweep_train, count=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
